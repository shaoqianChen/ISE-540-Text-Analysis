{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn # make sure this is installed in your environment.\n",
    "from sklearn.datasets import *\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This homework assumes that you have completed hw-1 and are familiar 'enough' with Python by now. Our main goal\n",
    "in this homework is to play with sklearn, which is the primary toolkit in Python that is used for basic machine\n",
    "learning like the models we studied in class (Naive Bayes, decision trees, linear classifiers and regressors).\n",
    "\n",
    "For our experiments, we will use a dataset publicly available in the UCI repository, namely the Bank Marketing Data Set.\n",
    "While this is not a 'text' dataset, as we studied in class, the classification models/supervised learning tend to involve the same\n",
    "kinds of workflow. This dataset is easier to work with than a text dataset because of the presence of numeric attributes (not requiring\n",
    "you to really know 'vector space models' in detail).\n",
    "\n",
    "Go to the website to read more: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "All files mentioned/referenced below can be found at this link.\n",
    "\n",
    "Note that we will be using the bank-additional-full.csv file for our experiments. This is the most complete\n",
    "dataset (i.e. has all 41,000+ examples and 20 attributes). It is a binary classification task (the very last column,\n",
    "which is either a yes or a no). However, as you'll see, even before we can get to all the good model fitting\n",
    "stuff, there's some data cleaning/processing that we'll need to do first.\n",
    "\n",
    "In python we often use 'pass' as a placeholder. Wherever you see 'pass', it's a sign that you should replace with\n",
    "your own code (which could span multiple lines).\n",
    "\n",
    "If it makes you comfortable to define additional functions or data structures to help you, go for it.\n",
    "\n",
    "Do not forget to 'call' the functions as necessary, as you proceed with the assignment.\n",
    "\n",
    "HINT: The first part of this exercise (in my opinion) is the most time-consuming, despite technically being\n",
    "the easiest, just like in actual analytics projects (in practice, about 80-90% time\n",
    "in machine learning/data science projects in the real-world are taken up by 'data cleaning/wrangling'). I realize\n",
    " that you may want to work on some of the other stuff before being able to parse the file into a matrix. One piece of\n",
    " advice is that many of the other function rely on generic matrices X and y, which you can get from sklearn's\n",
    " sample datasets. I'll provide some guidance below. The important thing to remember is, you can  'test' the functions\n",
    " out of order, although to complete the assignment you will need to finish everything I ask you below.\n",
    "\n",
    " Perhaps a shorter way to state the hint above is, read the entire assignment before jumping on to it, and be\n",
    " strategic about how you allocate time to the various problems. Don't get frustrated!\n",
    "\n",
    " (Another) HINT: Make sure you have imported everything you need at the top of this file!\n",
    "\n",
    " Total possible points are 100.\n",
    "\n",
    "Good luck!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [30 points] Complete encode_record_into_vector and parse_file_into_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "    the goal of this function is to take a record (informally, a 'row' in your file)\n",
    "    and 'encode' it numerically so that we can actually do\n",
    "    machine learning with it. How you do the encoding is up to you, but one recommended way is to:\n",
    "    --leave the numeric variables (like age) as is\n",
    "    --assign a 'one hot encoding' to each possible value of a `categorical' variable. For example, for the categorical\n",
    "    variable 'loan', we have three possible values ('no', 'yes', 'unknown'). Since there are three possible values,\n",
    "    you can do a 'one hot encoding' by using three binary 'variables'. We may want to assign 'no' to 0 0 1,\n",
    "    'yes' to 0 1 0  and 'unknown' to 1 0 0 (you can see now why it's called one hot encoding).\n",
    "    :param record: I highly recommend that record be a 'dict', but you can use other data structures (like 'list') if\n",
    "    you want. For example,\n",
    "    you could even just pass in a string! However, the output MUST be a numeric vector.\n",
    "    Also, we will not do 'type' checking on this function, so you don't need to waste time checking for exceptions,\n",
    "    or worrying that we will try to trip you up by running your code using weird inputs.\n",
    "    The only requirement is that code you write must work for the dataset used in this assignment.\n",
    "    :return:\n",
    "    x: the vector representation of the record\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_record_into_vector(record):\n",
    "    \n",
    "    def job_helper(x):\n",
    "        job = OneHotEncoder()\n",
    "        l=array(['admin.','blue-collar','entrepreneur','housemaid','management',\n",
    "             'retired','self-employed','services','student','technician'\n",
    "             ,'unemployed','unknown']).reshape(-1, 1) \n",
    "        job.fit(l)\n",
    "        X_encod = job.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod.tolist()\n",
    "\n",
    "    def marital_helper(x):\n",
    "        marital = OneHotEncoder()\n",
    "        l=array(['divorced','married','single', 'unknown']).reshape(-1, 1) \n",
    "        marital.fit(l)\n",
    "        X_encod = marital.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod\n",
    "\n",
    "    def education_helper(x):\n",
    "        education = OneHotEncoder()\n",
    "        l=array([\"basic.4y\",\"basic.6y\",\"basic.9y\",\n",
    "                 \"high.school\",\"illiterate\",\"professional.course\",\n",
    "                 \"university.degree\",\"unknown\"]).reshape(-1, 1) \n",
    "        education.fit(l)\n",
    "        X_encod = education.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod\n",
    "        \n",
    "    def default_helper(x):\n",
    "        default = OneHotEncoder()\n",
    "        l=array(['no','yes','unknown']).reshape(-1, 1) \n",
    "        default.fit(l)\n",
    "        X_encod = default.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod    \n",
    "    \n",
    "    def housing_helper(x):\n",
    "        housing = OneHotEncoder()\n",
    "        l=array(['no','yes', 'unknown']).reshape(-1, 1) \n",
    "        housing.fit(l)\n",
    "        X_encod = housing.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod\n",
    "    \n",
    "    def loan_helper(x):\n",
    "        loan = OneHotEncoder()\n",
    "        l=array(['no','yes','unknown']).reshape(-1, 1) \n",
    "        loan.fit(l)\n",
    "        X_encod = loan.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod\n",
    "    \n",
    "    def contact_helper(x):\n",
    "        contact = OneHotEncoder()\n",
    "        l=array(['cellular','telephone']).reshape(-1, 1) \n",
    "        contact.fit(l)\n",
    "        X_encod = contact.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod\n",
    "    \n",
    "    def month_helper(x):\n",
    "        month = OneHotEncoder()\n",
    "        l=array([ 'jan', 'feb', 'mar','apr','may','jun','jul','aug','sep','oct', 'nov', 'dec']).reshape(-1, 1) \n",
    "        month.fit(l)\n",
    "        X_encod = month.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod\n",
    "    \n",
    "    def day_of_week_helper(x):\n",
    "        day_of_week = OneHotEncoder()\n",
    "        l=array([ 'mon','tue','wed','thu','fri']).reshape(-1, 1) \n",
    "        day_of_week.fit(l)\n",
    "        X_encod = day_of_week.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod\n",
    "    \n",
    "    def poutcome_helper(x):\n",
    "        poutcome = OneHotEncoder()\n",
    "        l=array([ \"failure\",\"nonexistent\",\"success\"]).reshape(-1, 1) \n",
    "        poutcome.fit(l)\n",
    "        X_encod = poutcome.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod\n",
    "    \n",
    "    def y_helper(x):\n",
    "        y = OneHotEncoder()\n",
    "        l=array([ 'yes','no']).reshape(-1, 1) \n",
    "        y.fit(l)\n",
    "        X_encod = y.transform(array([x]).reshape(-1, 1)).toarray()\n",
    "        return X_encod\n",
    "    \n",
    "    x=array([])\n",
    "    x=np.append(x,record['age'])\n",
    "    x=np.append(x,job_helper(record['job']))\n",
    "    x=np.append(x,marital_helper(record['marital']))\n",
    "    x=np.append(x,education_helper(record['education']))\n",
    "    x=np.append(x,default_helper(record['default']))\n",
    "    x=np.append(x,housing_helper(record['housing']))\n",
    "    x=np.append(x,loan_helper(record['loan']))\n",
    "    x=np.append(x,contact_helper(record['contact']))\n",
    "    x=np.append(x,day_of_week_helper(record['day_of_week']))\n",
    "    x=np.append(x,month_helper(record['month']))\n",
    "    x=np.append(x,record['duration'])\n",
    "    x=np.append(x,record['campaign'])\n",
    "    x=np.append(x,record['pdays'])\n",
    "    x=np.append(x,record['previous'])\n",
    "    x=np.append(x,poutcome_helper(record['poutcome']))\n",
    "    x=np.append(x,record['emp.var.rate'])\n",
    "    x=np.append(x,record['cons.price.idx'])\n",
    "    x=np.append(x,record['cons.conf.idx'])\n",
    "    x=np.append(x,record['euribor3m'])\n",
    "    x=np.append(x,record['nr.employed'])\n",
    "    x=np.append(x,y_helper(record['y']))\n",
    "    \n",
    "    #pass  Hint: use helper functions to encode categorical variables as vectors that can be appended to the (bigger) x vector\n",
    "    # rather than write a lot of messy code here. It will also help you to try more things.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "record=dict()\n",
    "record['age']=58\n",
    "record['job']='management'\n",
    "record['marital']='married'\n",
    "record['education']='basic.4y'\n",
    "record['default']='no'\n",
    "record['housing']='yes'\n",
    "record['loan']='no'\n",
    "record['contact']='cellular'\n",
    "record['day_of_week']='thu'\n",
    "record['month']='may'\n",
    "record['duration']=261\n",
    "record['campaign']=1\n",
    "record['pdays']=-1\n",
    "record['previous']=0\n",
    "record['poutcome']='nonexistent'\n",
    "record['emp.var.rate']=1.1\n",
    "record['cons.price.idx']=93.994\n",
    "record['cons.conf.idx']=-36.4\n",
    "record['euribor3m']=4.857\n",
    "record['nr.employed']=5191\n",
    "record['y']='yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.8000e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6100e+02,  1.0000e+00,\n",
       "       -1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "        1.1000e+00,  9.3994e+01, -3.6400e+01,  4.8570e+00,  5.1910e+03,\n",
       "        0.0000e+00,  1.0000e+00])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_record_into_vector(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Hint 1: Even though the file is a .csv, be careful about using Python's csv package for reading in the file.\n",
    "    The single biggest source of error is not reading in the file correctly. It's good to test that you're\n",
    "    doing the initial data processing correctly on the first few records.\n",
    "\n",
    "    Hint 2: This function will make a call to encode_record_ for each record (not including header) in your input file\n",
    "    :param file_name: The path to the bank-additional-full.csv file\n",
    "    :return:\n",
    "    X: A D X p matrix, where D is the number of 'instances' or records in the file and p is the dimensionality of\n",
    "    the x vector that is output by the previous function. I won't give you the value for p; you have to know what it is\n",
    "    based on your code in encode_record_into_vector\n",
    "    y: A D X 1 matrix containing only 1's (for 'yes' in the output variable) or 0's (for 'no')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_into_matrix(file_name):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    with open(file_name, mode='r') as infile:\n",
    "        reader=csv.reader(infile)\n",
    "        header=next(reader)\n",
    "        if header != None:\n",
    "            for row in reader:\n",
    "                x = row[0].split(\";\")\n",
    "                record=dict()\n",
    "                record['age']=int(x[0])\n",
    "                record['job']=x[1][1:-1]\n",
    "                record['marital']=x[2][1:-1]\n",
    "                record['education']=x[3][1:-1]\n",
    "                record['default']=x[4][1:-1]\n",
    "                record['housing']=x[5][1:-1]\n",
    "                record['loan']=x[6][1:-1]\n",
    "                record['contact']=x[7][1:-1]\n",
    "                record['month']=x[8][1:-1]\n",
    "                record['day_of_week']=x[9][1:-1]\n",
    "                record['duration']=int(x[10])\n",
    "                record['campaign']=int(x[11])\n",
    "                record['pdays']=int(x[12])\n",
    "                record['previous']=int(x[13])\n",
    "                record['poutcome']=x[14][1:-1]\n",
    "                record['emp.var.rate']=float(x[15])\n",
    "                record['cons.price.idx']=float(x[16])\n",
    "                record['cons.conf.idx']=float(x[17])\n",
    "                record['euribor3m']=float(x[18])\n",
    "                record['nr.employed']=float(x[19])\n",
    "                record['y']=x[20][1:-1]\n",
    "                temp1=encode_record_into_vector(record).tolist()\n",
    "                X.append(temp1)\n",
    "                if record['y']=='yes':\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "We will define 'positive' instances as those with label 1, and negative instances as those with label 0.\n",
    "\n",
    "Q1 [5 points]. Write some code to count the number of 1s and 0s in y. How many positive and negative instances each\n",
    "are in your dataset?\n",
    "ANS:4640 1s and 36548 0s\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4640"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36548"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you want to do the next part before the previous part, I recommend calling X_y_for_running_tests below,\n",
    "which reads in a sample dataset from sklearn. I've included a short snippet of code below for your reference. Remember, however, that\n",
    "to score points, you must work with the banking dataset in the output you submit.\n",
    "\"\"\"\n",
    "\n",
    "def X_y_for_running_tests():\n",
    "    # just call this, and it will return X and y.\n",
    "    X, y = load_digits(2, True) # return only two classes, although there are ten total\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  1. ...  3.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  5. ...  8.  1.  0.]\n",
      " [ 0.  0.  6. ...  4.  0.  0.]\n",
      " [ 0.  0.  6. ...  6.  0.  0.]]\n",
      "now printing y\n",
      "[0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0\n",
      " 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1\n",
      " 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0\n",
      " 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0\n",
      " 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0\n",
      " 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1\n",
      " 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0\n",
      " 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1\n",
      " 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "X, y=X_y_for_running_tests()\n",
    "print (X)\n",
    "print ('now printing y')\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Here's an opportunity to test your sampling skills  using numpy. We want to do stratified sampling on X, y. Basically,\n",
    "     this means that we want to 'split' the original X, y (the 'complete' dataset) into a training dataset (X_train, y_train)\n",
    "     that will be used for training the model, and a testing dataset (X_test, y_test) that will be used for evaluating\n",
    "     the model. Here are the requirements:\n",
    "     --Since the sampling is stratified, we want to make sure that the proportion of positive instances (to total instances)\n",
    "     is equal in both training and testing data. For example:\n",
    "        Imagine that you had 100 positive instances and 50 negative instances in your full dataset. Suppose the training_Ratio\n",
    "        is 80%, as specified by default in the signature. Then, we want to randomly sample 0.8*100 positive instances (or 80\n",
    "        instances) and 0.8*50 negative instances (or 40 instances) and place all 120 instances in X_train (correspondingly, y_train\n",
    "        is filled with 1s and 0s based on the label). The remaining 30 instances (20 positive and 10 negative) are\n",
    "        placed in X_test (with y_test populated with 1s and 0s correspondingly). Notice that the ratio of positive\n",
    "        to positive+negative is equal in both training and testing datasets (compute it for yourself), and by extension,\n",
    "        so is the ratio of negative to positive+negative. There is a very good reason why this is so important (recall\n",
    "        the definition of learning in our very first slide! Would the test data still be from the same population as the\n",
    "        training data if we did not decide to 'stratify' the sample in this way?)\n",
    "     --Your method should work for any numeric choice of X, y and training ratio (that is between 0 and 1, including the extreme\n",
    "     cases of 0 and 1). We may test this function with X, y and training_ratio values of our own!\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_testing_split(X, y, training_ratio):\n",
    "    X_train, y_train, X_test, y_test=[],[],[],[]\n",
    "    X_1, X_0=[],[]\n",
    "    for i in X:\n",
    "        if (i[-1]==1) & (i[-2]==0):\n",
    "            X_1.append(i[:-2])\n",
    "        else:\n",
    "            X_0.append(i[:-2])\n",
    "    list1=random.sample(range(len(X_1)),int(len(X_1)*training_ratio))\n",
    "    list0=random.sample(range(len(X_0)),int(len(X_0)*training_ratio))\n",
    "    for p in range(len(X_1)):\n",
    "        if p in list1:\n",
    "            X_train.append(X_1[p])\n",
    "            y_train.append(1)\n",
    "        else:\n",
    "            X_test.append(X_1[p])\n",
    "            y_test.append(1)\n",
    "    for q in range(len(X_0)):\n",
    "        if q in list0:\n",
    "            X_train.append(X_0[q])\n",
    "            y_train.append(0)\n",
    "        else:\n",
    "            X_test.append(X_0[q])\n",
    "            y_test.append(0)\n",
    "    pass\n",
    "    return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2 (20 points). Run the code above with the X and y that you got from parse_file_into_matrix, with training ratios\n",
    "of 0.8, 0.5, 0.3 and 0.1. For each of these four cases, what is the 'ratio' of positive instances in the training\n",
    "dataset to the total number of instances in the training dataset? Verify that this same ratio is achieved in the test\n",
    "dataset. Write additional code to run these verifications if necessary (5 points per case).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test=training_testing_split(X, y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11265553869499241"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.count(1)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11264870114105366"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.count(1)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, y_train1, X_test1, y_test1=training_testing_split(X, y, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11265417111780131"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.count(1)/len(y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11265417111780131"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1.count(1)/len(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, y_train2, X_test2, y_test2=training_testing_split(X, y, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11265781806409841"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2.count(1)/len(y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11265260821309656"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test2.count(1)/len(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3, y_train3, X_test3, y_test3=training_testing_split(X, y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11267605633802817"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train3.count(1)/len(y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11265173995144322"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test3.count(1)/len(y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    In the code below, I have trained a model specifically for decision tree. You must expand the code to accommodate\n",
    "    the other two models. To learn more about sklearn's decision trees, see https://scikit-learn.org/stable/modules/tree.html\n",
    "    :param X_train: self-explanatory\n",
    "    :param y_train:\n",
    "    :param model: we will allow three values for model namely 'decision_tree', 'naive_bayes' and 'linear_SGD_classifier'\n",
    "    (Hint: you must change the loss function in https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "    to squared_loss to correctly implement the linear classifier. For the naive bayes, the appropriate model to use\n",
    "    is the Bernoulli naive Bayes.)\n",
    "\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    [10 points] Expand/replace 'pass' above to return the other two models based on the value of the model parameter 'model' [10 points]\n",
    "    [5 points] Expand to return one other model that I have not taught in class.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train,y_train,model):\n",
    "    if model == 'decision_tree':\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        return clf\n",
    "    if model == 'naive_bayes':\n",
    "        clf=BernoulliNB()\n",
    "        clf=clf.fit(X_train, y_train)\n",
    "    if model == 'linear_SGD_classifier':\n",
    "        clf = make_pipeline(StandardScaler(),SGDClassifier(loss=\"squared_loss\"))\n",
    "        clf=clf.fit(X_train, y_train)\n",
    "    if model == 'gradient_tree_boosting':\n",
    "        clf=GradientBoostingClassifier()\n",
    "        clf=clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Note that the model here is different from model in train_models. Here, we will be passing in the 'actual' trained\n",
    "    model and using it to evaluate X_test and y_test\n",
    "\n",
    "    We will be using the F_measure metric to evaluate the model: I have already written code in compute_f_measure\n",
    "     that takes your predicted y, produced in this function, as well as the 'true' y, which is y_test, and will\n",
    "     return a number to you within 0 and 1. The higher the f-measure, the better. I will briefly explain in class\n",
    "     why we prefer f-measure over accuracy in many ML tasks.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for this function has already been written\n",
    "\"\"\"\n",
    "def evaluate_model(X_test, y_test, model):\n",
    "    y_predict = model.predict(X_test)\n",
    "    return sklearn.metrics.f1_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[20 points] Compile a 5-table report with 10 rows per table. You must submit this report with your assignment submission\n",
    "(any reasonable format is fine). Each table will correspond to a value of training percent, specifically 10%,30%,50%,70%,90%\n",
    "Each table will contain four columns (trial number ranging from 1-10, decision_tree, naive_bayes and linear_classifier).\n",
    "In each cell, you will be reporting the f-measure achieved. Completing this question will also require you to complete\n",
    "the code for trials() below. trials() does NOT HAVE to (although it could) directly produce the table(s) but must give you all the information\n",
    "you need, or write information out to file, that is needed for you to populate the tables.\n",
    "\n",
    "Hint: It is okay to produce and print out intermediate outputs to file if necessary. For example, if you want\n",
    "you could print out the 'encoded' version of the dataset (which only has numbers) to file, and just read that in\n",
    "to save on time. Also, if there are some things that are constant, it is perfectly fine to define those constants\n",
    "or fix variable values outside trials. As always, I want to be reasonable in evaluating these things; the goal is not\n",
    "to trip you up or make you follow every instruction to the letter.\n",
    "\n",
    "[15 points] Now we will return to some statistics (you can use whatever tool you want, or even do it manually!). You should\n",
    "ignore the linear classifier for this question. Our null hypothesis is that the decision tree is better than naive bayes.\n",
    "(or our alternate hypothesis is that the naive bayes is better than decision tree). For each of the five training percentages,\n",
    "using 95% as the confidence level, can you reject the null hypothesis? State your p-values here for all five training\n",
    "percentages. Which test did you use?\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trials(file_name, training_ratio):\n",
    "    \"\"\"\n",
    "    Think of this as the 'main' or master function from which you will be calling all other code. You must modify\n",
    "    this code appropriately for your experiments (e.g., to vary training percentage, write out results to file etc.)\n",
    "    I'm not giving you any hints or placeholders for this function; by now, you should be familiar with how to do\n",
    "    what I've asked here!\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X, y=parse_file_into_matrix(file_name)\n",
    "    data=dict()\n",
    "    data['trial_number']=[1,2,3,4,5,6,7,8,9,10]\n",
    "    l_dr,l_nb,l_lc=[],[],[]\n",
    "    for i in range(10):\n",
    "        X_train, y_train, X_test, y_test=training_testing_split(X, y, training_ratio)\n",
    "        \n",
    "        #decision tree\n",
    "        clf=train_models(X_train,y_train,'decision_tree')\n",
    "        l_nb.append(evaluate_model(X_test, y_test, clf))\n",
    "        \n",
    "        #dnaive_bayes\n",
    "        clf=train_models(X_train,y_train,'naive_bayes')\n",
    "        l_dr.append(evaluate_model(X_test, y_test, clf))\n",
    "        \n",
    "        #linear_classifier\n",
    "        clf=train_models(X_train,y_train,'linear_SGD_classifier')\n",
    "        l_lc.append(evaluate_model(X_test, y_test, clf))\n",
    "     \n",
    "    data['decision_tree']=l_dr\n",
    "    data['naive_bayes']=l_nb\n",
    "    data['linear_SGD_classifier']=l_lc\n",
    "    \n",
    "    df=pd.DataFrame (data, columns = ['trial_number','decision_tree','naive_bayes','linear_SGD_classifier'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10%,30%,50%,70%,90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_number</th>\n",
       "      <th>decision_tree</th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>linear_SGD_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.380438</td>\n",
       "      <td>0.504333</td>\n",
       "      <td>0.175858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.373743</td>\n",
       "      <td>0.510231</td>\n",
       "      <td>0.210546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.373492</td>\n",
       "      <td>0.499882</td>\n",
       "      <td>0.177422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.367047</td>\n",
       "      <td>0.488631</td>\n",
       "      <td>0.163442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.369614</td>\n",
       "      <td>0.507393</td>\n",
       "      <td>0.140942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.385105</td>\n",
       "      <td>0.515144</td>\n",
       "      <td>0.154673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.385022</td>\n",
       "      <td>0.475478</td>\n",
       "      <td>0.180563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.383674</td>\n",
       "      <td>0.504861</td>\n",
       "      <td>0.168085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.370766</td>\n",
       "      <td>0.509497</td>\n",
       "      <td>0.207347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.369381</td>\n",
       "      <td>0.485435</td>\n",
       "      <td>0.117835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_number  decision_tree  naive_bayes  linear_SGD_classifier\n",
       "0             1       0.380438     0.504333               0.175858\n",
       "1             2       0.373743     0.510231               0.210546\n",
       "2             3       0.373492     0.499882               0.177422\n",
       "3             4       0.367047     0.488631               0.163442\n",
       "4             5       0.369614     0.507393               0.140942\n",
       "5             6       0.385105     0.515144               0.154673\n",
       "6             7       0.385022     0.475478               0.180563\n",
       "7             8       0.383674     0.504861               0.168085\n",
       "8             9       0.370766     0.509497               0.207347\n",
       "9            10       0.369381     0.485435               0.117835"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.1 traing ratio\n",
    "df1=trials('bank-additional-full.csv', 0.1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_number</th>\n",
       "      <th>decision_tree</th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>linear_SGD_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.384745</td>\n",
       "      <td>0.517777</td>\n",
       "      <td>0.174388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.379644</td>\n",
       "      <td>0.505751</td>\n",
       "      <td>0.164932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.373979</td>\n",
       "      <td>0.511036</td>\n",
       "      <td>0.182615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.369402</td>\n",
       "      <td>0.513551</td>\n",
       "      <td>0.189139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.376892</td>\n",
       "      <td>0.505017</td>\n",
       "      <td>0.156499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.379395</td>\n",
       "      <td>0.507553</td>\n",
       "      <td>0.186499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.377393</td>\n",
       "      <td>0.496450</td>\n",
       "      <td>0.251081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.371248</td>\n",
       "      <td>0.513497</td>\n",
       "      <td>0.134763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.364283</td>\n",
       "      <td>0.505772</td>\n",
       "      <td>0.153213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.377830</td>\n",
       "      <td>0.514338</td>\n",
       "      <td>0.109481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_number  decision_tree  naive_bayes  linear_SGD_classifier\n",
       "0             1       0.384745     0.517777               0.174388\n",
       "1             2       0.379644     0.505751               0.164932\n",
       "2             3       0.373979     0.511036               0.182615\n",
       "3             4       0.369402     0.513551               0.189139\n",
       "4             5       0.376892     0.505017               0.156499\n",
       "5             6       0.379395     0.507553               0.186499\n",
       "6             7       0.377393     0.496450               0.251081\n",
       "7             8       0.371248     0.513497               0.134763\n",
       "8             9       0.364283     0.505772               0.153213\n",
       "9            10       0.377830     0.514338               0.109481"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.3 traing ratio\n",
    "df2=trials('bank-additional-full.csv', 0.3)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_number</th>\n",
       "      <th>decision_tree</th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>linear_SGD_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.372170</td>\n",
       "      <td>0.518363</td>\n",
       "      <td>0.208238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.374292</td>\n",
       "      <td>0.506797</td>\n",
       "      <td>0.134276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.369202</td>\n",
       "      <td>0.503196</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.373671</td>\n",
       "      <td>0.508944</td>\n",
       "      <td>0.158737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.377222</td>\n",
       "      <td>0.516520</td>\n",
       "      <td>0.183402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.378190</td>\n",
       "      <td>0.512689</td>\n",
       "      <td>0.109070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.376328</td>\n",
       "      <td>0.500324</td>\n",
       "      <td>0.204524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.375812</td>\n",
       "      <td>0.508190</td>\n",
       "      <td>0.150877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.369111</td>\n",
       "      <td>0.508330</td>\n",
       "      <td>0.136587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.369030</td>\n",
       "      <td>0.509170</td>\n",
       "      <td>0.147007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_number  decision_tree  naive_bayes  linear_SGD_classifier\n",
       "0             1       0.372170     0.518363               0.208238\n",
       "1             2       0.374292     0.506797               0.134276\n",
       "2             3       0.369202     0.503196               0.207500\n",
       "3             4       0.373671     0.508944               0.158737\n",
       "4             5       0.377222     0.516520               0.183402\n",
       "5             6       0.378190     0.512689               0.109070\n",
       "6             7       0.376328     0.500324               0.204524\n",
       "7             8       0.375812     0.508190               0.150877\n",
       "8             9       0.369111     0.508330               0.136587\n",
       "9            10       0.369030     0.509170               0.147007"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.5 traing ratio\n",
    "df3=trials('bank-additional-full.csv', 0.5)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_number</th>\n",
       "      <th>decision_tree</th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>linear_SGD_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.389179</td>\n",
       "      <td>0.511644</td>\n",
       "      <td>0.149139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.379809</td>\n",
       "      <td>0.540222</td>\n",
       "      <td>0.213370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.373894</td>\n",
       "      <td>0.502146</td>\n",
       "      <td>0.170302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.376963</td>\n",
       "      <td>0.491699</td>\n",
       "      <td>0.238138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.376110</td>\n",
       "      <td>0.520489</td>\n",
       "      <td>0.142669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.371676</td>\n",
       "      <td>0.517035</td>\n",
       "      <td>0.141257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.513250</td>\n",
       "      <td>0.434659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.369136</td>\n",
       "      <td>0.503451</td>\n",
       "      <td>0.204580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.362724</td>\n",
       "      <td>0.517528</td>\n",
       "      <td>0.139265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.359620</td>\n",
       "      <td>0.515967</td>\n",
       "      <td>0.193335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_number  decision_tree  naive_bayes  linear_SGD_classifier\n",
       "0             1       0.389179     0.511644               0.149139\n",
       "1             2       0.379809     0.540222               0.213370\n",
       "2             3       0.373894     0.502146               0.170302\n",
       "3             4       0.376963     0.491699               0.238138\n",
       "4             5       0.376110     0.520489               0.142669\n",
       "5             6       0.371676     0.517035               0.141257\n",
       "6             7       0.359189     0.513250               0.434659\n",
       "7             8       0.369136     0.503451               0.204580\n",
       "8             9       0.362724     0.517528               0.139265\n",
       "9            10       0.359620     0.515967               0.193335"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.7 traing ratio\n",
    "df4=trials('bank-additional-full.csv', 0.7)\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Users/madison_w/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_number</th>\n",
       "      <th>decision_tree</th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>linear_SGD_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.181893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.368087</td>\n",
       "      <td>0.547718</td>\n",
       "      <td>0.170654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.362117</td>\n",
       "      <td>0.519957</td>\n",
       "      <td>0.169647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.363148</td>\n",
       "      <td>0.496774</td>\n",
       "      <td>0.217210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.355917</td>\n",
       "      <td>0.503735</td>\n",
       "      <td>0.182909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.380425</td>\n",
       "      <td>0.533762</td>\n",
       "      <td>0.201348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.380866</td>\n",
       "      <td>0.480851</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.333935</td>\n",
       "      <td>0.522560</td>\n",
       "      <td>0.147326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.393352</td>\n",
       "      <td>0.517423</td>\n",
       "      <td>0.180655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.353357</td>\n",
       "      <td>0.486545</td>\n",
       "      <td>0.180102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_number  decision_tree  naive_bayes  linear_SGD_classifier\n",
       "0             1       0.380952     0.531646               0.181893\n",
       "1             2       0.368087     0.547718               0.170654\n",
       "2             3       0.362117     0.519957               0.169647\n",
       "3             4       0.363148     0.496774               0.217210\n",
       "4             5       0.355917     0.503735               0.182909\n",
       "5             6       0.380425     0.533762               0.201348\n",
       "6             7       0.380866     0.480851               0.173913\n",
       "7             8       0.333935     0.522560               0.147326\n",
       "8             9       0.393352     0.517423               0.180655\n",
       "9            10       0.353357     0.486545               0.180102"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.9 traing ratio\n",
    "df5=trials('bank-additional-full.csv', 0.9)\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('0.1.csv', index=False)\n",
    "df2.to_csv('0.3.csv', index=False)\n",
    "df3.to_csv('0.5.csv', index=False)\n",
    "df4.to_csv('0.7.csv', index=False)\n",
    "df5.to_csv('0.9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
